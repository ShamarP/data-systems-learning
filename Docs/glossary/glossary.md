Language models: encodes statistical information about one of more languages. Can figure out what word is more likely to show up next given context.

Tokenization: The process of breaking original text into tokens

Masked Language model: Trained to predict missing tokens anywhere in a sequence. Useful in sentiment analysis and text classification.

Autoregressive language model: Trained to predict next token in a sequence. Used for text generation.

Self-supervised learning: Can learn from text sequences without labeling. As labels are inferred 

Supervised Learning: requires labels to train models

